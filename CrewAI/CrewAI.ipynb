{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI with Ollama\n",
    "\n",
    "This Jupyter notebook demonstrates the use of CrewAI in conjunction with the Ollama language model. \n",
    "\n",
    "The notebook outlines the creation of various agents and tasks for content preparation, specifically for a YouTube channel focused on topics such as AI, Machine Learning, and Neural Networks.\n",
    "\n",
    "[Video of the Live Session](https://youtu.be/5hzFOYjH6XI)\n",
    "\n",
    "[CrewAI](https://crewai.com/) | [Ollama](https://ollama.com/) | [LangChain](https://langchain.com/) | [Agentops](https://agentops.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U crewai langchain langchain_community arxiv wikipedia\n",
    "%pip install -U agentops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell installs the necessary packages for the notebook, including crewai, langchain, langchain_community, arxiv, wikipedia.\n",
    "\n",
    "The %%capture command suppresses the output of the installation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task\n",
    "from textwrap import dedent\n",
    "import os\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "# optional\n",
    "from agentops.langchain_callback_handler import LangchainCallbackHandler as AgentOpsLangchainCallbackHandler\n",
    "agentops_handler = AgentOpsLangchainCallbackHandler(api_key=os.environ.get('AGENTOPS_API_KEY'), tags=['CrewAI Example'])\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:latest\",\n",
    "    #callbacks=[agentops_handler],\n",
    "    n_ctx=4098\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example chain \n",
    "#prompt_contribute = PromptTemplate.from_template(\"\"\"\\\n",
    "#You are a {Role}; \n",
    "#{BackStory}\n",
    "#Your goal is :{Goal}\n",
    "#Using {Tools}\n",
    "#\"\"\")\n",
    "#chain  = prompt_contribute | llm | StrOutputParser()\n",
    "#chain.invoke(\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the necessary libraries and modules. \n",
    "\n",
    "These include CrewAI's Agent and Task, dedent from textwrap, various components from langchain, and display utilities from IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentPreparationTasks():\n",
    "    def research_task(self, agent, topics, subject):\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\\\n",
    "                Conduct comprehensive research on each topic already covered by my channel. Gather information on recent\n",
    "                news, trends and research papers about the Channel Topic.\n",
    "                Identify potential gaps and new directions for future content.\n",
    "                        \n",
    "                Topics: {topics}\n",
    "                Channel Subject: {subject}\"\"\"),\n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                A detailed report summarizing new topics and directions to consider\n",
    "                for new content relevant to the channel subject.\"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent, \n",
    "        )\n",
    "\n",
    "    def outline_creation_task(self, agent, new_topic, subject):\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\\\n",
    "                Analyze the current industry trends, challenges, and opportunities\n",
    "                relevant to the channel subject. Consider research papers, reports, recent\n",
    "                developments, and expert opinions to provide a comprehensive\n",
    "                overview of the {new_topic} for my channel about: {subject}.\n",
    "                Identify major trends, potential challenges, and future advancements.\"\"\"),\n",
    "            expected_output=dedent(f\"\"\"\\\n",
    "                An insightful and engaging outline for content on my channel about '{subject}' that identifies major trends, potential\n",
    "                challenges, and future advancements about {new_topic}.\"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent, \n",
    "        )\n",
    "\t\n",
    "    def cross_reference_content_task(self, agent):\n",
    "        return Task(\n",
    "            description=dedent(\"\"\"\\\n",
    "                Analyze the current topics and the outline of the topic to create a comprehensive\n",
    "                list of cross-references, and connections between the new topics and the existing ones.\n",
    "                This will help in creating a cohesive and well-structured narrative.\n",
    "\n",
    "                Topics: {topics}\n",
    "                New Topic: {new_topic}\n",
    "                New Outline: {new_outline}\n",
    "            \"\"\"),\n",
    "                \n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                An comprehensive list of cross-references and connections between the new topics.\n",
    "                \"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent, \n",
    "        )\n",
    "\n",
    "    def script_writing_task(self, agent):\n",
    "        return Task(\n",
    "            description=dedent(\"\"\"\\\n",
    "                Develop a detailed script based on the provided outline. Ensure the script\n",
    "                is engaging, informative, and aligns with the overall tone and style of the channel.\n",
    "                Incorporate any relevant research findings and expert opinions to add depth and credibility.\n",
    "                \n",
    "                ##Topic \n",
    "                {new_topic}\n",
    "                \n",
    "                ## Outline\n",
    "                {outline}\n",
    "\n",
    "                ## Channel Subject\n",
    "                {subject}\n",
    "            \"\"\"),\n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                A complete and polished script ready for production, including all necessary dialogue, transitions,\n",
    "                and references to research and expert opinions, using markdown format.\"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent,\n",
    "        )\n",
    "\n",
    "    def visual_asset_task(self, agent):\n",
    "        return Task(\n",
    "            description=dedent(\"\"\"\\\n",
    "                Identify and create visual assets required for the video based on the script.\n",
    "                This includes images, graphics, charts, and any other visual elements that\n",
    "                will enhance the storytelling and provide clear, engaging content for viewers.\n",
    "\n",
    "                Script: {script}\n",
    "                Channel Subject: {subject}\n",
    "            \"\"\"),\n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                A collection of prompts to create visual assets, including images, graphics, charts, and other visual elements, ready to be\n",
    "                incorporated into the video production process.\"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent,\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def create_twitter_post_task(self, agent, video_url):\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\\\n",
    "                Create an engaging Twitter post to promote the newly published video. The post should include a compelling\n",
    "                description, relevant hashtags, and a link to the video. \n",
    "\n",
    "                Video URL: {video_url}\n",
    "                Channel Subject: {{subject}}\n",
    "            \"\"\"),\n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                A ready-to-publish Twitter post that includes an engaging description, relevant hashtags, and a link to the video.\"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent\n",
    "        )\n",
    "\n",
    "    def create_linkedin_post_task(self, agent, video_url):\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\\\n",
    "                Create an engaging LinkedIn post to promote the newly published video. The post should include a professional\n",
    "                description, relevant hashtags, and a link to the video. Consider the LinkedIn audience and tailor the content\n",
    "                to suit a professional network.\n",
    "\n",
    "                Video URL: {video_url}\n",
    "                Channel Subject: {{subject}}\n",
    "            \"\"\"),\n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                A ready-to-publish LinkedIn post that includes a professional description, relevant hashtags, and a link to the video.\"\"\"),\n",
    "            async_execution=False,\n",
    "            agent=agent\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class defines various tasks related to content preparation, such as research, outline creation, script writing, visual asset creation, and social media promotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Research Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Research Papers on the Topic\n",
       "\n",
       "Published: 2020-10-21\n",
       "Title: Dying ReLU and Initialization: Theory and Numerical Examples\n",
       "Authors: Lu Lu, Yeonjong Shin, Yanhui Su, George Em Karniadakis\n",
       "Summary: The dying ReLU refers to the problem when ReLU neurons become inactive and\n",
       "only output 0 for any input. There are many empirical and heuristic\n",
       "explanations of why ReLU neurons die. However, little is known about its\n",
       "theoretical analysis. In this paper, we rigorously prove that a deep ReLU\n",
       "network will eventually die in probability as the depth goes to infinite.\n",
       "Several methods have been proposed to alleviate the dying ReLU. Perhaps, one of\n",
       "the simplest treatments is to modify the initialization procedure. One common\n",
       "way of initializing weights and biases uses symmetric probability\n",
       "distributions, which suffers from the dying ReLU. We thus propose a new\n",
       "initialization procedure, namely, a randomized asymmetric initialization. We\n",
       "prove that the new initialization can effectively prevent the dying ReLU. All\n",
       "parameters required for the new initialization are theoretically designed.\n",
       "Numerical examples are provided to demonstrate the effectiveness of the new\n",
       "initialization procedure.\n",
       "\n",
       "Published: 2024-03-12\n",
       "Title: Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons\n",
       "Authors: Simon Dufort-Labbé, Pierluca D'Oro, Evgenii Nikishin, Razvan Pascanu, Pierre-Luc Bacon, Aristide Baratin\n",
       "Summary: When training deep neural networks, the phenomenon of $\\textit{dying\n",
       "neurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero\n",
       "during training$\\unicode{x2013}$ has traditionally been viewed as undesirable,\n",
       "linked with optimization challenges, and contributing to plasticity loss in\n",
       "continual learning scenarios. In this paper, we reassess this phenomenon,\n",
       "focusing on sparsity and pruning. By systematically exploring the impact of\n",
       "various hyperparameter configurations on dying neurons, we unveil their\n",
       "potential to facilitate simple yet effective structured pruning algorithms. We\n",
       "introduce $\\textit{Demon Pruning}$ (DemP), a method that controls the\n",
       "proliferation of dead neurons, dynamically leading to network sparsity.\n",
       "Achieved through a combination of noise injection on active units and a\n",
       "one-cycled schedule regularization strategy, DemP stands out for its simplicity\n",
       "and broad applicability. Experiments on CIFAR10 and ImageNet datasets\n",
       "demonstrate that DemP surpasses existing structured pruning techniques,\n",
       "showcasing superior accuracy-sparsity tradeoffs and training speedups. These\n",
       "findings suggest a novel perspective on dying neurons as a valuable resource\n",
       "for efficient model compression and optimization.\n",
       "\n",
       "Published: 2017-12-07\n",
       "Title: Solving internal covariate shift in deep learning with linked neurons\n",
       "Authors: Carles Roger Riera Molina, Oriol Pujol Vila\n",
       "Summary: This work proposes a novel solution to the problem of internal covariate\n",
       "shift and dying neurons using the concept of linked neurons. We define the\n",
       "neuron linkage in terms of two constraints: first, all neuron activations in\n",
       "the linkage must have the same operating point. That is to say, all of them\n",
       "share input weights. Secondly, a set of neurons is linked if and only if there\n",
       "is at least one member of the linkage that has a non-zero gradient in regard to\n",
       "the input of the activation function. This means that for any input in the\n",
       "activation function, there is at least one member of the linkage that operates\n",
       "in a non-flat and non-zero area. This simple change has profound implications\n",
       "in the network learning dynamics. In this article we explore the consequences\n",
       "of this proposal and show that by using this kind of units, internal covariate\n",
       "shift is implicitly solved. As a result of this, the use of linked neurons\n",
       "allows to train arbitrarily large networks without any architectural or\n",
       "algorithmic trick, effectively removing the need of using re-normalization\n",
       "schemes such as Batch Normalization, which leads to halving the required\n",
       "training time. It also solves the problem of the need for stan"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Tool for the Arxiv API.\"\"\"\n",
    "\n",
    "from typing import Optional, Type\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForToolRun\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "\n",
    "\n",
    "class ArxivInput(BaseModel):\n",
    "    \"\"\"Input for the Arxiv tool.\"\"\"\n",
    "\n",
    "    query: str = Field(description=\"search query to look up\")\n",
    "    max_results: int = Field(description=\"maximum number of results to return\", default=5)\n",
    "\n",
    "\n",
    "# you have ArxivQueryRun: taking as arguments: query and max_results\n",
    "#\"A wrapper around Arxiv.org \"\n",
    "#\"Useful for when you need to answer questions about Artificial Inteligence, Machine Physics, Mathematics, \"\n",
    "#\"Computer Science, Quantitative Biology, Quantitative Finance, Statistics, \"\n",
    "#\"Electrical Engineering, and Economics \"\n",
    "#\"from scientific articles on arxiv.org. \"\n",
    "#\"Input should be a search query using parameter query.\"\n",
    "\n",
    "# param: query: str = \"cat=cs.AI and dying neurons\"\n",
    "# param: max_results: int = 5\n",
    "\n",
    "\n",
    "\n",
    "class ArxivQueryRun(BaseTool):\n",
    "    \"\"\"Tool that searches the Arxiv API.\"\"\"\n",
    "\n",
    "    name: str = \"arxiv\"\n",
    "    description: str = (\n",
    "        \"A wrapper around Arxiv.org \"\n",
    "        \"Useful for when you need to answer questions about Physics, Mathematics, \"\n",
    "        \"Computer Science, Quantitative Biology, Quantitative Finance, Statistics, \"\n",
    "        \"Electrical Engineering, and Economics \"\n",
    "        \"from scientific articles on arxiv.org. \"\n",
    "        \"Input should be a search query using parameter query.\"\n",
    "    )\n",
    "    api_wrapper: ArxivAPIWrapper = Field(default_factory=ArxivAPIWrapper)\n",
    "    args_schema: Type[BaseModel] = ArxivInput\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the Arxiv tool.\"\"\"\n",
    "        return self.api_wrapper.run(query)\n",
    "arxiv = ArxivQueryRun() \n",
    "\n",
    "docs = arxiv.run(\"cat=cs.AI and dying neurons\")\n",
    "docs\n",
    "display(Markdown(f\"## Research Papers on the Topic\\n\\n{docs}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Research Papers on the Topic\n",
       "\n",
       "Page: Backpropagation\n",
       "Summary: In machine learning, backpropagation is a gradient estimation method used to train neural network models. The gradient estimate is used by the optimization algorithm to compute the network parameter updates.\n",
       "It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.\n",
       "Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\n",
       "Strictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.\n",
       "\n",
       "Page: Backpropagation through time\n",
       "Summary: Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.\n",
       "\n",
       "\n",
       "\n",
       "Page: Neural backpropagation\n",
       "Summary: Neural backpropagation is the phenomenon in which, after the action potential of a neuron creates a voltage spike down the axon (normal propagation), another impulse is generated from the soma and propagates towards the apical portions of the dendritic arbor or dendrites (from which much of the original input current originated).  In addition to active backpropagation of the action potential, there is also passive electrotonic spread. While there is ample evidence to prove the existence of backpropagating action potentials, the function of such action potentials and the extent to which they invade the most distal dendrites remain highly controversial.\n",
       "\n",
       "Page: Almeida–Pineda recurrent backpropagation\n",
       "Summary: Almeida–Pineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks. It is a type of supervised learning. It was described somewhat cryptically in Richard Feynman's senior thesis, and rediscovered independently in the context of artificial neural networks by both Fernando Pineda and Luis B. Almeida.\n",
       "A recurrent neural network for this algorithm consists of some input units, some output units and eventually some hidden units.\n",
       "For a given set of (input, target) states, the network is trained to settle into a stable activation state with the output units in the target state, based on a given input state clamped on the input units.\n",
       "\n",
       "Page: History of artificial neural networks\n",
       "Summary: Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics, the first implementation of ANNs was by psychologist Frank Rosenblatt, who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s, with the AAAI calling that period an \"AI winter\". \n",
       "Later, advances in hardware and the development of the backpropagation algorithm as well a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=5))\n",
    "\n",
    "wikipedia_docs = wikipedia.run(\"backpropagation algorithms\")\n",
    "display(Markdown(f\"## Research Papers on the Topic\\n\\n{wikipedia_docs}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tools for agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'C:\\\\Users\\\\sebastiendg\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " ArxivQueryRun()]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "tools = load_tools(\n",
    "    [\"wikipedia\"],\n",
    ") \n",
    "\n",
    "tools = tools + [arxiv]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentPreparationAgents:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    # Define the agents involved in the content creation process\n",
    "    def create_researcher(self,tools=[]):\n",
    "        return Agent(\n",
    "            role=\"Research Agent\",\n",
    "\n",
    "            backstory=\"\"\"\\\n",
    "            Ray was a former investigative journalist who has a knack for uncovering the latest trends and insights.\n",
    "            After years of digging deep into stories, Ray transitioned into the digital content world, leveraging his skills to help content creators stay ahead of the curve.\"\"\",\n",
    "            \n",
    "            goal=\"To provide comprehensive and up-to-date research that helps shape the direction of the channel’s content.\",\n",
    "            llm=self.llm,\n",
    "            \n",
    "            tools=tools,\n",
    "            allow_delegation=False,\n",
    "\t\t\tverbose=True\n",
    "        )\n",
    "\n",
    "    def create_outline_writer(self,tools=[]):\n",
    "        return Agent(\n",
    "            role=\"Outline Creation Agent\",\n",
    "            backstory=\"\"\"\\\n",
    "            Olivia was once a strategy consultant for major media firms, known for her ability to synthesize complex information into clear, actionable plans.\n",
    "            She now uses her talents to help content creators map out their content effectively.\"\"\",\n",
    "\n",
    "            goal=\"To create detailed and engaging outlines that guide the content creation process seamlessly.\",\n",
    "            llm=self.llm,\n",
    "            tools=tools,\n",
    "            allow_delegation=False,\n",
    "\t\t\tverbose=True\n",
    "    )\n",
    "    \n",
    "    def create_content_agent(self, tools=[]):\n",
    "        return Agent(\n",
    "            role=\"Content Integration Agent\",\n",
    "\n",
    "            backstory=\"\"\"\\\n",
    "            Chris has a background in library sciences and data management.\n",
    "            His expertise in organizing and cross-referencing large volumes of information makes him perfect for ensuring new content fits well within existing frameworks.\"\"\",\n",
    "            \n",
    "            goal=\"To build a cohesive narrative by connecting new content with existing topics.\",\n",
    "\n",
    "            tools=tools,\n",
    "            llm=self.llm,\n",
    "            allow_delegation=False,\n",
    "\t\t\tverbose=True\n",
    "        )\n",
    "\n",
    "    def create_script_writer(self,tools=[]):\n",
    "        return Agent(\n",
    "        role=\"Script Writing Agent\",\n",
    "        backstory=\"\"\"\\\n",
    "            Sam is a former screenwriter with experience in both film and digital media.\n",
    "            His storytelling skills ensure that every piece of content is engaging and informative.\"\"\",\n",
    "\n",
    "        goal=\"To craft scripts that are compelling and aligned with the channel's voice and vision.\",\n",
    "        tools=tools,\n",
    "        llm=self.llm,\n",
    "        allow_delegation=False,\n",
    "\t\tverbose=True\n",
    "    )\n",
    "\n",
    "    def create_visual_asset_creator(self,tools=[]):\n",
    "        return Agent(\n",
    "        role=\"Visual Asset Creation Agent\",\n",
    "\n",
    "        backstory=\"\"\"\\\n",
    "        Vicky is a graphic designer and visual artist who has worked with numerous brands to create eye-catching visuals.\n",
    "        Her ability to translate concepts into compelling visuals and their prompt is unparalleled.\"\"\",\n",
    "        \n",
    "        goal=\"To create visual assets that enhance the storytelling of the content.\",\n",
    "        tools=tools,\n",
    "        llm=self.llm,\n",
    "        allow_delegation=False,\n",
    "\t\tverbose=True\n",
    "    )\n",
    "\n",
    "    def create_social_media_agent(self,tools=[]):\n",
    "        return Agent(\n",
    "        role=\"Social Media and Community Engagement Agent\",\n",
    "\n",
    "        backstory=\"\"\"\\\n",
    "        Casey is a social media strategist who has successfully managed online communities for various brands.\n",
    "        With expertise in crafting engaging posts and interacting with audiences, Casey ensures that the content reaches and resonates with a wide audience.\"\"\",\n",
    "\n",
    "        goal=\"To create and schedule engaging posts for Twitter and LinkedIn that promote the new video content effectively.\",\n",
    "        tools=tools,\n",
    "        llm=self.llm,\n",
    "        allow_delegation=False,\n",
    "\t\tverbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class defines various agents involved in content preparation, such as content agent, \n",
    "outline writer, researcher, script writer, visual asset creator, and social media agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew\n",
    "\n",
    "tasks = ContentPreparationTasks()\n",
    "agents = ContentPreparationAgents(llm)\n",
    "\n",
    "content_agent = agents.create_content_agent(tools)\n",
    "outline_writer = agents.create_outline_writer(tools)\n",
    "researcher = agents.create_researcher(tools)\n",
    "script_writer = agents.create_script_writer(tools)\n",
    "visual_asset_creator = agents.create_visual_asset_creator(tools)\n",
    "social_media_agent = agents.create_social_media_agent(tools)\n",
    "\n",
    "youtuber_crew = Crew(\n",
    "    agents=[\n",
    "        researcher,\n",
    "        content_agent,\n",
    "        outline_writer,\n",
    "        researcher,\n",
    "        script_writer,\n",
    "        visual_asset_creator,\n",
    "        social_media_agent\n",
    "    ], \n",
    "    tasks=[\n",
    "        tasks.research_task(researcher, \"AI, Machine Learning, Neural Networks\", \"Artificial Intelligence\"),\n",
    "\n",
    "        tasks.outline_creation_task(outline_writer, \"Backpropagation Algorithms\", \"Artificial Intelligence\"),\n",
    "            \n",
    "        tasks.cross_reference_content_task(content_agent),\n",
    "\n",
    "        tasks.script_writing_task(script_writer),\n",
    "\n",
    "        tasks.visual_asset_task(visual_asset_creator),\n",
    "\n",
    "        tasks.create_twitter_post_task(social_media_agent, \n",
    "            \"https://youtube.com/video\"),\n",
    "        tasks.create_linkedin_post_task(social_media_agent,\n",
    "             \"https://youtube.com/video\")\n",
    "    ],\n",
    "    manager=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section initializes the tasks and agents, assigns specific tasks to the respective agents, and creates a crew of agents managed by the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "youtuber_crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command starts the process, activating the agents to perform their assigned tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
