{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection Data Preparation\n",
    "\n",
    "This notebook demonstrates the steps to prepare the GoEmotions dataset for a multi-label classification task.\n",
    "\n",
    " We will perform the following steps:\n",
    "1. Group and sum emotion labels for each unique text.\n",
    "2. Clean the dataset by removing conflicting neutral labels.\n",
    "3. Retain only the top 3 emotion labels per text.\n",
    "4. Stratify the dataset based on the labels.\n",
    "5. Visualize the data before and after each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "First, we load the GoEmotions dataset and inspect its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataFrame = load_dataset('go_emotions', 'raw')\n",
    "\n",
    "print(\"Total data size: \", len(dataFrame[\"train\"]))\n",
    "df = dataFrame[\"train\"].to_pandas()\n",
    "df.to_parquet(\"go_emotions.parquet\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspecting the text column\n",
    "\n",
    "* We want to look at the text column and determine the max_sequence_length our model will have to handle.\n",
    "* And to avoid data-leakage between test and validation we also want to deduplicate the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "df[\"length_text\"] = df[\"text\"].apply(len)\n",
    "df[\"length_text\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df[\"length_text\"], bins=150, kde=True)\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rows with text length lower than 10\n",
    "df[df[\"length_text\"] < 3][\"text\"].unique()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rows with text length larget than 200\n",
    "df[df[\"length_text\"] > 200][\"text\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# suppress < 3 and > 256\n",
    "df = df[(df[\"length_text\"] >= 3) & (df[\"length_text\"] <= 200)]\n",
    "# plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df[\"length_text\"], bins=50, kde=True)\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Group and Sum Emotion Labels\n",
    "Next, we group the dataset by text and sum the emotion labels for each unique text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns containing emotion labels\n",
    "emotions = df.columns[9:-1]\n",
    "\n",
    "# Group by text and sum the emotion columns\n",
    "df_grouped = df.groupby('text', as_index=False)[emotions].sum()\n",
    "\n",
    "# Display the first few rows of the grouped dataset\n",
    "print(\"Grouped Dataset:\")\n",
    "print(df_grouped.head())\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix displaying the counts of each emotion\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.heatmap(df_grouped[emotions].corr(), annot=True, fmt=\".1f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame from the labels\n",
    "labels_df = df_grouped[emotions]\n",
    "\n",
    "# Calculate the co-occurrence matrix\n",
    "co_occurrence_matrix = labels_df.T.dot(labels_df)\n",
    "\n",
    "# Convert to a DataFrame for better readability\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix, columns=labels_df.columns, index=labels_df.columns)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(co_occurrence_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Label Co-Occurrence Matrix')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Remove Conflicting Neutral Labels\n",
    "We remove the neutral label if it is set amongst other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_except_neutral = emotions[:-1]\n",
    "all_except_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times neutral is present along with other emotions\n",
    "df_grouped[\"neutral_only\"] = df_grouped[all_except_neutral].sum(axis=1) == 0\n",
    "df_grouped[\"neutral_and_other\"] = df_grouped[\"neutral_only\"] == False\n",
    "print(\"Neutral Only:\", df_grouped[\"neutral_only\"].sum())\n",
    "print(\"Neutral and Other:\", df_grouped[\"neutral_and_other\"].sum())\n",
    "# number of items with neutral above 1 \n",
    "df_grouped[\"neutral_above_1\"] = df_grouped[\"neutral\"] > 1\n",
    "print(\"Neutral above 1:\", df_grouped[\"neutral_above_1\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the dataset by removing the neutral label if it conflicts with other labels\n",
    "def remove_conflicting_neutral(row):\n",
    "    if row['neutral'] > 0 and row[all_except_neutral].sum() > 0:\n",
    "        row['neutral'] = 0\n",
    "    return row\n",
    "\n",
    "df_grouped = df_grouped.apply(remove_conflicting_neutral, axis=1)\n",
    "\n",
    "# Display the first few rows after removing conflicting neutral labels\n",
    "print(\"Dataset after Removing Conflicting Neutral Labels:\")\n",
    "# count the number of times neutral is present along with other emotions\n",
    "df_grouped[\"neutral_only\"] = df_grouped[all_except_neutral].sum(axis=1) == 0\n",
    "df_grouped[\"neutral_and_other\"] = df_grouped[\"neutral_only\"] == False\n",
    "print(\"Neutral Only:\", df_grouped[\"neutral_only\"].sum())\n",
    "print(\"Neutral and Other:\", df_grouped[\"neutral_and_other\"].sum())\n",
    "# number of items with neutral above 1 \n",
    "df_grouped[\"neutral_above_1\"] = df_grouped[\"neutral\"] > 1\n",
    "print(\"Neutral above 1:\", df_grouped[\"neutral_above_1\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retain Only Top 3 Labels\n",
    "For each row, we keep only the top 3 emotion labels with the highest scores and set those to 1, while setting all other labels to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to keep only the top 3 labels\n",
    "def keep_top_labels(row, top_labels=3):\n",
    "    top_indices = row.nlargest(top_labels).index\n",
    "    # only keep those labels where the value is greater than 0\n",
    "    top_indices = top_indices[row[top_indices] > 0]\n",
    "    row[:] = 0\n",
    "    row[top_indices] = 1\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "df_grouped[emotions] = df_grouped[emotions].apply(lambda l: keep_top_labels(l, 3), axis=1)\n",
    "\n",
    "# Display the first few rows after keeping only the top 3 labels\n",
    "print(\"Dataset after Keeping Only Top Labels:\")\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratify the Dataset\n",
    "We stratify the dataset based on the labels to ensure the distribution of labels is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# shuffle the dataset\n",
    "df_grouped = df_grouped.sample(frac=1, random_state=42)\n",
    "\n",
    "# Create a multi-label indicator for stratification\n",
    "df_grouped['labels'] = df_grouped[emotions].apply(lambda x: tuple(x), axis=1)\n",
    "\n",
    "# Count the occurrences of each label combination\n",
    "label_counts = df_grouped['labels'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of label combinations occuring only once\n",
    "single_label_counts = label_counts[label_counts == 1].count()\n",
    "print(f\"Label combinations occuring only once: {single_label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify rare combinations (those that appear less than twice)\n",
    "rare_combinations = label_counts[label_counts < 2].index\n",
    "print(\"Rare Combinations:\", len(rare_combinations))\n",
    "\n",
    "# Filter out rows with rare combinations\n",
    "df_filtered = df_grouped[~df_grouped['labels'].isin(rare_combinations)]\n",
    "\n",
    "# Stratify the dataset\n",
    "train, test = train_test_split(df_filtered, test_size=0.1, random_state=42, shuffle=True,  stratify=df_filtered['labels'])\n",
    "\n",
    "# Drop the 'labels' column as it was only needed for stratification\n",
    "train = train.drop(columns=['labels'])\n",
    "test = test.drop(columns=['labels'])\n",
    "\n",
    "# Display the resulting datasets\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset by oversampling the minority classes\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Initialize the RandomOverSampler\n",
    "oversampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Fit and apply the sampler\n",
    "X_resampled, y_resampled = oversampler.fit_resample(df_grouped[emotions], df_grouped['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Resampled dataset size:\", len(X_resampled), len(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "Let's visualize the distribution of labels before and after each step.\n",
    "\n",
    "### Original Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original distribution of emotion labels\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "original_emotion_counts = df[emotions].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=original_emotion_counts.index, y=original_emotion_counts.values, palette=\"viridis\")\n",
    "plt.title('Original Counts per Emotion in Go Emotions Dataset')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of emotion labels after removing conflicting neutral labels\n",
    "cleaned_emotion_counts = df_grouped[emotions].sum().sort_values(ascending=False)\n",
    "\n",
    "# plot histogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "cleaned_emotion_counts.plot(kind='bar', color='skyblue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with Top 3 Labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emotion counts for the training set\n",
    "train_emotion_counts = train[emotions].sum()\n",
    "\n",
    "# Calculate emotion counts for the test set\n",
    "test_emotion_counts = test[emotions].sum()\n",
    "\n",
    "# Combine the counts into a DataFrame\n",
    "emotion_counts_df = pd.DataFrame({\n",
    "    'Train': train_emotion_counts,\n",
    "    'Test': test_emotion_counts\n",
    "})\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Emotion Counts in Train and Test Sets:\")\n",
    "print(emotion_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "emotion_counts_df.plot(kind='bar', stacked=True, figsize=(12, 8), color=['skyblue', 'lightgreen'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Stacked Distribution of Emotions in Train and Test Sets')\n",
    "plt.xlabel('Emotions')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Stacked Distribution of Emotions in Train and Test Sets\n",
    "Finally, let's visualize the distribution of emotions in the training and test sets using a stacked bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emotion counts for the training set\n",
    "train_emotion_counts = train[emotions].sum()\n",
    "\n",
    "# Calculate emotion counts for the test set\n",
    "test_emotion_counts = test[emotions].sum()\n",
    "\n",
    "# Combine the counts into a DataFrame\n",
    "emotion_counts_df = pd.DataFrame({\n",
    "    'Train': train_emotion_counts,\n",
    "    'Test': test_emotion_counts\n",
    "})\n",
    "\n",
    "# Calculate total counts for sorting\n",
    "emotion_counts_df['Total'] = emotion_counts_df['Train'] + emotion_counts_df['Test']\n",
    "\n",
    "# Sort the DataFrame by total counts\n",
    "emotion_counts_df = emotion_counts_df.sort_values(by='Total', ascending=False)\n",
    "\n",
    "# Drop the total column for plotting\n",
    "emotion_counts_df = emotion_counts_df.drop(columns=['Total'])\n",
    "\n",
    "# Display the sorted combined DataFrame\n",
    "print(\"Sorted Emotion Counts in Train and Test Sets:\")\n",
    "print(emotion_counts_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "emotion_counts_df.plot(kind='bar', stacked=True, figsize=(12, 8), color=['skyblue', 'lightgreen'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Stacked Distribution of Emotions in Train and Test Sets (Sorted)')\n",
    "plt.xlabel('Emotions')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"go_emotions_train.parquet\")\n",
    "test.to_parquet(\"go_emotions_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
