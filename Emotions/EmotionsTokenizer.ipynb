{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a fundamental step in Natural Language Processing (NLP) that involves splitting text into smaller units called tokens. These tokens can be words, subwords, or characters, and they are the building blocks for processing textual data in machine learning models.  The tokenizers library by HuggingFace offers a fast and efficient way to tokenize text, handling large datasets and integrating seamlessly with the transformers library.\n",
    "\n",
    "However, tokenization can be challenging, particularly when dealing with punctuation and special characters. \n",
    "\n",
    "Standard tokenizers often split text at punctuation marks, which can lead to the loss of meaningful tokens, such as emoticons (e.g., :), ;)) and specific emoji representations (e.g., :thumbsup:).\n",
    "\n",
    "\n",
    "Here I just highlight a very specific problem to this particular use case but see Andrej Karpathy's amaizing video about tokenization for a much more in depth take on the matter\n",
    "\n",
    "https://www.youtube.com/watch?v=zduSFxRajkE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/sebdg/.local/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: keras in /home/sebdg/.local/lib/python3.10/site-packages (3.3.3)\n",
      "Requirement already satisfied: pandas in /home/sebdg/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/sebdg/.local/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.6.7)\n",
      "Requirement already satisfied: transformers in /home/sebdg/.local/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: datasets in /home/sebdg/.local/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: emoji in /home/sebdg/.local/lib/python3.10/site-packages (2.11.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: packaging in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (70.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (1.64.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: namex in /home/sebdg/.local/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/sebdg/.local/lib/python3.10/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: rich in /home/sebdg/.local/lib/python3.10/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sebdg/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sebdg/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sebdg/.local/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sebdg/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/sebdg/.local/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: click in /home/sebdg/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sebdg/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sebdg/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sebdg/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/sebdg/.local/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/sebdg/.local/lib/python3.10/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: xxhash in /home/sebdg/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/sebdg/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sebdg/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/sebdg/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sebdg/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sebdg/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sebdg/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sebdg/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sebdg/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sebdg/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sebdg/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sebdg/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sebdg/.local/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sebdg/.local/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sebdg/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sebdg/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow keras pandas scikit-learn nltk transformers datasets emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Smileys Punctuation in Tokenization\n",
    "\n",
    "The main issue with smileys in tokenization is that most tokenizers treat them as punctuation marks as delimiters.\n",
    "\n",
    "This means that sequences like :) or ;) might be split into separate tokens, which can alter their intended meaning. \n",
    "\n",
    "For example, :) could be tokenized into :, ), or even removed entirely, losing the smileys's semantic value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using tensorflow.keras Tokenizer\n",
    "\n",
    "Lets create a custom tokenizer and test it on two different sentiments :\n",
    "    \n",
    "```text\n",
    "Oh what a day :)\n",
    "Oh what a day :(\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh what a day :) ==> [2, 3, 4, 5]\n",
      "Oh what a day :( ==> [2, 3, 4, 5]\n",
      "Are equal :  True\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"Oh what a day :)\",\n",
    "    \"Oh what a day :(\",\n",
    "]\n",
    "\n",
    "# create a tokenizer\n",
    "bad_tokenizer = Tokenizer(num_words=256, oov_token=\"<UNK>\")\n",
    "\n",
    "# train the tokenizer on the sentences\n",
    "bad_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# tokenize the sentences\n",
    "tokenized =  bad_tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(sentences[0], \"==>\", tokenized[0])\n",
    "print(sentences[1], \"==>\", tokenized[1])\n",
    "\n",
    "# the two sentences are not equal but the tokenized versions are...\n",
    "# hmmm we've just lost the meaning in the process\n",
    "print(\"Are equal : \",   tokenized[0] == tokenized[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try to Add them to the Vocaulary\n",
    "\n",
    "By adding them to the vocabulary we would expect the tokenizer to now consider them as their own tokens.\n",
    "\n",
    "So lets try that out... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'oh': 2,\n",
       " 'what': 3,\n",
       " 'a': 4,\n",
       " 'day': 5,\n",
       " ':)': 6,\n",
       " ';)': 7,\n",
       " ':P': 8,\n",
       " ':D': 9,\n",
       " ':(': 10,\n",
       " \":'(\": 11,\n",
       " ':O': 12,\n",
       " ':/': 13,\n",
       " ':|': 14,\n",
       " ':*': 15,\n",
       " ':@': 16,\n",
       " '>:(': 17}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_tokens = [\n",
    "    # smileys\n",
    "    \":)\", \";)\", \":P\", \":D\", \":(\", \":'(\", \":O\", \":/\", \":|\", \":*\", \":@\", \">:(\", \n",
    "]\n",
    "\n",
    "\n",
    "bad_tokenizer = Tokenizer(num_words=256, oov_token=\"<UNK>\")\n",
    "bad_tokenizer.fit_on_texts(sentences)\n",
    "bad_tokenizer.word_index.update({token: len(bad_tokenizer.word_index) + i + 1 for i, token in enumerate(additional_tokens)})\n",
    "\n",
    "# lets take a look at our word index\n",
    "bad_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh what a day :) ==> [2, 3, 4, 5]\n",
      "Oh what a day :( ==> [2, 3, 4, 5]\n",
      "Are equal :  True  <--- arfff... they are equal again\n"
     ]
    }
   ],
   "source": [
    "# The above word index looks good, we have assigned a unique index to each smiley\n",
    "\n",
    "bad_tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "\n",
    "print(sentences[0], \"==>\", tokenized[0])\n",
    "print(sentences[1], \"==>\", tokenized[1])\n",
    "\n",
    "# the two sentences are not equal but the tokenized versions are...\n",
    "# hmmm we've just lost the meaning in the process\n",
    "print(\"Are equal : \",   tokenized[0] == tokenized[1], \" <--- arfff... they are equal again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the problem remained the same despesite our attempt to specialize the vocabulary... Why ? \n",
    "\n",
    "Because ; : and ( ) are considered as punctuation and separators between workds and they are not tokenized themselves, this tokenizer considers them equivalent to whitespace..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1 : TextPreprocessing\n",
    "\n",
    "To address this, one effective approach is to preprocess the text by substituting special tokens with placeholders before tokenization.\n",
    "\n",
    "This ensures that these tokens are treated as single units and preserved during the tokenization process.\n",
    "\n",
    "Preprocessing involves scanning the text for special tokens and replacing them with unique placeholders.\n",
    "\n",
    "These placeholders are then tokenized as single units. After tokenization, the placeholders can be mapped back to their original forms if needed.\n",
    "\n",
    "This method uses regular expressions (regex) to identify and replace the tokens efficiently.\n",
    "\n",
    "Steps:\n",
    "1. Define Special Tokens: List all special tokens (e.g., :), ;), :thumbsup:).\n",
    "2. Create Placeholders: Generate unique placeholders for each special token.\n",
    "3. Replace Tokens with Placeholders: Use regex to substitute special tokens in the text with their corresponding placeholders.\n",
    "4. Tokenize: Apply the tokenizer to the preprocessed text.\n",
    "5. Map Placeholders Back: Optionally, convert placeholders back to the original tokens after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  {'<UNK>': 1, 'oh': 2, 'what': 3, 'a': 4, 'day': 5, 'spt0': 6, 'spt4': 7}\n",
      "Preprocessed :  ['Oh what a day <|SPT0|>', 'Oh what a day <|SPT4|>']\n",
      "Tokenized :  [[2, 3, 4, 5, 6], [2, 3, 4, 5, 7]]\n",
      "Are equal :  False  <--- seams like now we now if you had a good or bad day\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "\n",
    "# Assuming additional_tokens is already defined\n",
    "\n",
    "# Define a function to preprocess texts and preserve special tokens\n",
    "def preprocess_texts(texts, additional_tokens):\n",
    "    token_dict = {token: f\"<|SPT{i}|>\" for i, token in enumerate(additional_tokens)}\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(token) for token in additional_tokens) + r')')\n",
    "    \n",
    "    def replace_tokens(text):\n",
    "        return pattern.sub(lambda match: token_dict[match.group(0)], text)\n",
    "    \n",
    "    preprocessed_texts = [replace_tokens(text) for text in texts]\n",
    "    return preprocessed_texts, token_dict\n",
    "\n",
    "# Prepare the tokenizer\n",
    "tokenizer = Tokenizer(num_words=100000, oov_token=\"<UNK>\")\n",
    "\n",
    "# Preprocess the additional tokens to ensure they are preserved\n",
    "preprocessed_additional_tokens, token_dict = preprocess_texts(sentences, additional_tokens)\n",
    "\n",
    "# Fit the tokenizer on the preprocessed additional tokens\n",
    "tokenizer.fit_on_texts(preprocessed_additional_tokens)\n",
    "\n",
    "# Add the additional tokens to the tokenizer's word index with their original form\n",
    "for token, placeholder in token_dict.items():\n",
    "    if placeholder in tokenizer.word_index:\n",
    "        tokenizer.word_index[token] = tokenizer.word_index.pop(placeholder)\n",
    "\n",
    "\n",
    "print(\"Vocabulary : \", tokenizer.word_index)\n",
    "\n",
    "# Preprocess the sample texts\n",
    "preprocessed_sample_texts, _ = preprocess_texts(sentences, additional_tokens)\n",
    "print(\"Preprocessed : \", preprocessed_sample_texts)\n",
    "# Tokenize the preprocessed sample texts\n",
    "tokenized = tokenizer.texts_to_sequences(preprocessed_sample_texts)\n",
    "\n",
    "# Print the tokenized sequences\n",
    "print(\"Tokenized : \", tokenized)\n",
    "\n",
    "print(\"Are equal : \",   tokenized[0] == tokenized[1], \" <--- seams like now we now if you had a good or bad day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2 : Using the Transformers Library\n",
    "\n",
    "The transformers library by HuggingFace provides robust tools for tokenization, including the ability to add and preserve custom tokens. \n",
    "\n",
    "Here's how you can use the transformers library to handle special tokens effectively:\n",
    "\n",
    "This solution will reuse an already trained tokenizer so we don't have to find enough text to ensure there are not too much <UNK> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2821, 2054, 1037, 2154, 30522, 102]\n",
      "[101, 2821, 2054, 1037, 2154, 30526, 102]\n",
      "Are equal :  False  <--- seams like now we now if you had a good or bad day\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Add additional tokens to the tokenizer\n",
    "tokenizer.add_tokens(additional_tokens)\n",
    "\n",
    "# Tokenize sample texts\n",
    "tokenized_texts = tokenizer(sentences, is_split_into_words = False)\n",
    "\n",
    "\n",
    "# Print the tokenized sequences\n",
    "print(tokenized_texts[0].ids)\n",
    "print(tokenized_texts[1].ids)\n",
    "\n",
    "print(\"Are equal : \",   tokenized[0] == tokenized[1], \" <--- seams like now we now if you had a good or bad day\")\n",
    "\n",
    "# note that the tokenized sequences now also start with [CLS] and end with [SEP] tokens to indicate the beginning and end of the sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
